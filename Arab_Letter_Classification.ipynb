{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fc61370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torcheval.metrics.functional import multiclass_f1_score\n",
    "from torcheval.metrics.functional import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72f3ffd-5d81-498a-bd43-4a56a4c207b0",
   "metadata": {},
   "source": [
    "Setting up file paths and some other things for converting image data to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03617556",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path ='./archive/Train Images 13440x32x32/train/'\n",
    "test_path ='./archive/Test Images 3360x32x32/test/'\n",
    "\n",
    "\n",
    "train_files=os.listdir(training_path)\n",
    "test_files=os.listdir(test_path)\n",
    "transform=transforms.Compose([transforms.ToTensor()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6dfb36d6-631b-4856-8354-3a8438041c63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_builder(path, files):\n",
    "    '''Function for converting image data to tensors for using in training, along with getting the labels\n",
    "       from the file name.\n",
    "    '''\n",
    "    img_fordim=cv2.imread(os.path.join(path,files[0]), 0)\n",
    "    image_size=torch.flatten(transform(img_fordim))\n",
    "    dim1=image_size.shape[0]\n",
    "    dim0=len(files)\n",
    "    data=torch.zeros((dim0, dim1))\n",
    "    labels=torch.zeros((dim0, 1))\n",
    "    for i, file in enumerate(files):\n",
    "        img=cv2.imread(os.path.join(path,file), 0)\n",
    "        flat_tens=torch.flatten(transform(img))\n",
    "        data[i]=flat_tens\n",
    "        label=file.split('_')[-1][:2]\n",
    "        labels[i]=float(label)-1\n",
    "        \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72f25d6-0fa4-4cbc-828b-935840f1e129",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "097d523e-579f-44d3-9636-b546c76c2933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device= 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6f8357f6-a244-44e6-b9c2-9d6929cc1bed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data, test_labels = data_builder(test_path, test_files)\n",
    "\n",
    "train_data, train_labels = data_builder(training_path, train_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "38cdb36d-5867-4b08-9186-323748036967",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(test_data, 'test_data.pt')\n",
    "torch.save(test_labels, 'test_labels.pt')\n",
    "torch.save(train_data, 'train_data.pt')\n",
    "torch.save(train_labels, 'train_labels.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "21937e6c-0159-485c-a19c-499cf7d2e4e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ArabicCharClassifier(nn.Module):\n",
    "    def __init__(self, n_char):\n",
    "        super().__init__()\n",
    "        self.hidden1=nn.Linear(1024, 100)\n",
    "        self.hidden2=nn.Linear(100, 100)\n",
    "        self.hidden3=nn.Linear(100,100)\n",
    "        self.hidden4=nn.Linear(100,100)\n",
    "        self.hidden5=nn.Linear(100, n_char)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.dropout=nn.Dropout()\n",
    "        self.batch1=nn.BatchNorm1d(100)\n",
    "        self.batch2=nn.BatchNorm1d(100)\n",
    "        self.batch3=nn.BatchNorm1d(100)\n",
    "        self.batch4=nn.BatchNorm1d(100)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        X = self.hidden1(x)\n",
    "        X = self.relu(X)\n",
    "        X = self.batch1(X)\n",
    "        X = self.dropout(X)\n",
    "        \n",
    "        X = self.hidden2(X)\n",
    "        X = self.relu(X)\n",
    "        X = self.batch2(X)\n",
    "        X = self.dropout(X)\n",
    "        \n",
    "        X = self.hidden3(X)\n",
    "        X = self.relu(X)\n",
    "        X = self.batch3(X)\n",
    "        X = self.dropout(X)\n",
    "        \n",
    "        X = self.hidden4(X)\n",
    "        X = self.relu(X)\n",
    "        X = self.batch4(X)\n",
    "        X = self.dropout(X)\n",
    "        \n",
    "        \n",
    "        X = self.hidden5(X)\n",
    "        \n",
    "        return X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "18daa46c-319d-4fe0-b99b-ccc27a133e11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ArabicCharClassifier(\n",
       "  (hidden1): Linear(in_features=1024, out_features=100, bias=True)\n",
       "  (hidden2): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (hidden3): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (hidden4): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (hidden5): Linear(in_features=100, out_features=28, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (batch1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch3): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch4): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=ArabicCharClassifier(n_char=28).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "640e1be1-9b9c-40e5-8f35-650a3572d7d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "optimization = optim.Adam(params=model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "35706cfd-0dbd-4e09-b42c-3fb230a9545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y_pred, y_true, num_char):\n",
    "    acc = multiclass_accuracy(y_pred, y_true)\n",
    "    f1_score = multiclass_f1_score(y_pred, y_true, num_classes=num_char)\n",
    "    return f1_score, acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e8992508-15db-4896-ba2d-325f13dd006b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_firsttry=model(test_data.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7bb90757-c2ca-4f29-afaf-4f03b7696514",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[17.],\n",
       "        [18.],\n",
       "        [18.],\n",
       "        [18.],\n",
       "        [18.]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "68abeabf-f3e3-4c56-82e8-0b05729ca495",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28.0000, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=nn.Softmax(dim=0)\n",
    "m(y_firsttry).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "06ef1872-a9aa-4ea3-bb2a-e7893cd21d10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Train loss: 3.65175 | Train accuracy: 3.393% | Train f1 0.034 | Test accuracy 5.268% | Test f1 0.053\n",
      "Epoch: 10 | Train loss: 2.82922 | Train accuracy: 14.903% | Train f1 0.149 | Test accuracy 15.119% | Test f1 0.151\n",
      "Epoch: 20 | Train loss: 2.00974 | Train accuracy: 34.055% | Train f1 0.341 | Test accuracy 28.095% | Test f1 0.281\n",
      "Epoch: 30 | Train loss: 1.52423 | Train accuracy: 47.932% | Train f1 0.479 | Test accuracy 47.292% | Test f1 0.473\n",
      "Epoch: 40 | Train loss: 1.08867 | Train accuracy: 61.838% | Train f1 0.618 | Test accuracy 58.631% | Test f1 0.586\n",
      "Epoch: 50 | Train loss: 1.04186 | Train accuracy: 62.619% | Train f1 0.626 | Test accuracy 56.726% | Test f1 0.567\n",
      "Epoch: 60 | Train loss: 0.74138 | Train accuracy: 73.452% | Train f1 0.735 | Test accuracy 64.851% | Test f1 0.649\n",
      "Epoch: 70 | Train loss: 0.54425 | Train accuracy: 80.484% | Train f1 0.805 | Test accuracy 66.994% | Test f1 0.670\n",
      "Epoch: 80 | Train loss: 0.40369 | Train accuracy: 86.347% | Train f1 0.863 | Test accuracy 68.899% | Test f1 0.689\n",
      "Epoch: 90 | Train loss: 0.27990 | Train accuracy: 90.848% | Train f1 0.908 | Test accuracy 69.762% | Test f1 0.698\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 100\n",
    "\n",
    "train_data, train_labels = train_data.to(device), train_labels.to(device)\n",
    "\n",
    "test_data, test_labels = test_data.to(device), test_labels.to(device)\n",
    "\n",
    "train_metrics={'loss': [], 'accuracy': [], 'f1_score': []}\n",
    "\n",
    "test_metrics = {'accuracy': [], 'f1_score': []}\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    \n",
    "    train_pred=model(train_data)\n",
    "    model.train()\n",
    "    \n",
    "    \n",
    "    \n",
    "    y_train = torch.squeeze(train_labels).type(torch.LongTensor) \n",
    "    \n",
    "    optimization.zero_grad()\n",
    "    \n",
    "    loss_epoch = loss(train_pred, y_train)\n",
    "    \n",
    "    acc_train, f1_train = metrics(train_pred, y_train, 28)\n",
    "    \n",
    "    loss_epoch.backward()\n",
    "    \n",
    "    optimization.step()\n",
    "    \n",
    "    train_metrics['loss'].append(loss_epoch)\n",
    "    train_metrics['accuracy'].append(acc_train*100)\n",
    "    train_metrics['f1_score'].append(f1_train)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        test_pred = model(test_data)\n",
    "        acc_test, f1_test = metrics(test_pred, torch.squeeze(test_labels), 28)\n",
    "        \n",
    "        test_metrics['accuracy'].append(acc_test*100)\n",
    "        test_metrics['f1_score'].append(f1_test)\n",
    "        \n",
    "    if epoch%10==0:\n",
    "        print(f'Epoch: {epoch} | Train loss: {loss_epoch:.5f} | Train accuracy: {acc_train*100:.3f}% | Train f1 {f1_train:.3f} | Test accuracy {acc_test*100:.3f}% | Test f1 {f1_test:.3f}')\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f4a4c2-fa4e-426a-816c-90ba84a5dd3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
